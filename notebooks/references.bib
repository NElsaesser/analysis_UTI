@misc{Gubian2025,
 author = {Gubian, Michele},
 year = {2025},
 title = {Analysis of Uni- and Multi-Dimensional Contours with GAMs and Functional PCA: an Application to Ultrasound Tongue Imaging},
 url = {https://github.com/uasolo/FPCA-phonetics-workshop/blob/master/presentations/UTI/UTI_analysis_code.html}
}


@misc{Coretta2025,
 abstract = {Prepare for submission.},
 author = {Coretta, Stefano and Sakr, Georges},
 year = {2025},
 title = {stefanocoretta/mv{\_}uti: v1.1},
 url = {https://stefanocoretta.github.io/mv_uti},
 publisher = {Zenodo},
 doi = {10.5281/zenodo.15335918},
 file = {Stefano Coretta, Georges Sakr 2025 - stefanocoretta mv{\_}uti:Attachments/Stefano Coretta, Georges Sakr 2025 - stefanocoretta mv{\_}uti.pdf:application/pdf}
}


@article{Wieling2018,
 author = {Wieling, Martijn},
 year = {2018},
 title = {Analyzing dynamic phonetic data using generalized additive mixed modeling: A tutorial focusing on articulatory differences between L1 and L2 speakers of English},
 pages = {86--116},
 volume = {70},
 issn = {00954470},
 journal = {Journal of Phonetics},
 doi = {10.1016/j.wocn.2018.03.002},
 file = {Wieling{\_}Analyzing dynamic phonetic data using generalized additive mixed:Attachments/Wieling{\_}Analyzing dynamic phonetic data using generalized additive mixed.pdf:application/pdf}
}

@article{Gubian2015,
 author = {Gubian, Michele and Torreira, Francisco and Boves, Lou},
 year = {2015},
 title = {Using Functional Data Analysis for investigating multidimensional dynamic phonetic contrasts},
 pages = {16--40},
 volume = {49},
 issn = {00954470},
 journal = {Journal of Phonetics},
 doi = {10.1016/j.wocn.2014.10.001},
 file = {Gubian{\_}Using Functional Data Analysis for investigating multidimensional dynamic phonetic contrasts:Attachments/Gubian{\_}Using Functional Data Analysis for investigating multidimensional dynamic phonetic contrasts.pdf:application/pdf}
}

@article{Wrench2022,
 abstract = {Automatic feature extraction from images of speech articulators is currently achieved by detecting edges. Here, we investigate the use of pose estimation deep neural nets with transfer learning to perform markerless estimation of speech articulator keypoints using only a few hundred hand-labelled images as training input. Midsagittal ultrasound images of the tongue, jaw, and hyoid and camera images of the lips were hand-labelled with keypoints, trained using DeepLabCut and evaluated on unseen speakers and systems. Tongue surface contours interpolated from estimated and hand-labelled keypoints produced an average mean sum of distances (MSD) of 0.93, s.d. 0.46 mm, compared with 0.96, s.d. 0.39 mm, for two human labellers, and 2.3, s.d. 1.5 mm, for the best performing edge detection algorithm. A pilot set of simultaneous electromagnetic articulography (EMA) and ultrasound recordings demonstrated partial correlation among three physical sensor positions and the corresponding estimated keypoints and requires further investigation. The accuracy of the estimating lip aperture from a camera video was high, with a mean MSD of 0.70, s.d. 0.56 mm compared with 0.57, s.d. 0.48 mm for two human labellers. DeepLabCut was found to be a fast, accurate and fully automatic method of providing unique kinematic data for tongue, hyoid, jaw, and lips.},
 author = {Wrench, Alan and Balch-Tomes, Jonathan},
 year = {2022},
 title = {Beyond the Edge: Markerless Pose Estimation of Speech Articulators from Ultrasound and Camera Images Using DeepLabCut},
 volume = {22},
 number = {3},
 journal = {Sensors (Basel, Switzerland)},
 doi = {10.3390/s22031133},
 file = {sensors-22-01133-v2:Attachments/sensors-22-01133-v2.pdf:application/pdf}
}





